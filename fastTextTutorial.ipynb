{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af28372",
   "metadata": {},
   "source": [
    "## fastText Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f4fd2",
   "metadata": {},
   "source": [
    "## Replication of original work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d984c",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "Text classification is a core problem to many applications, like spam detection, sentiment analysis or smart replies. In this tutorial, we describe how to build a text classifier with the fastText tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdefb68",
   "metadata": {},
   "source": [
    "## What is text classification?\n",
    "The goal of text classification is to assign documents (such as emails, posts, text messages, product reviews, etc...) to one or multiple categories. Such categories can be review scores, spam v.s. non-spam, or the language in which the document was typed. Nowadays, the dominant approach to build such classifiers is machine learning, that is learning classification rules from examples. In order to build such classifiers, we need labeled data, which consists of documents and their corresponding categories (or tags, or labels).\n",
    "\n",
    "As an example, we build a classifier which automatically classifies stackexchange questions about cooking into one of several possible tags, such as pot, bowl or baking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6fbff",
   "metadata": {},
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecf86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8f1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fasttext\n",
    "import fasttext\n",
    "#help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c2f43",
   "metadata": {},
   "source": [
    "train_supervised() will mostly be used for retruning a model object and calling test and predict on that object. This is the same as learning the text classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6a6ed",
   "metadata": {},
   "source": [
    "## Getting and Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11c381",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, we need labeled data to train our supervised classifier. In this tutorial, we are interested in building a classifier to automatically recognize the topic of a stackexchange question about cooking. Let's download examples of questions from the cooking section of Stackexchange, and their associated tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e04a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking.stackexchange.id\n",
      "cooking.stackexchange.txt\n",
      "readme.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-11-04 23:22:36--  https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "WARNING: cannot verify dl.fbaipublicfiles.com's certificate, issued by 'CN=Cloudflare Inc ECC CA-3,O=Cloudflare\\\\, Inc.,C=US':\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 457609 (447K) [application/x-tar]\n",
      "Saving to: 'cooking.stackexchange.tar.gz.3'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 11%  264K 2s\n",
      "    50K .......... .......... .......... .......... .......... 22%  354K 1s\n",
      "   100K .......... .......... .......... .......... .......... 33%  244K 1s\n",
      "   150K .......... .......... .......... .......... .......... 44% 2.84M 1s\n",
      "   200K .......... .......... .......... .......... .......... 55% 6.81M 0s\n",
      "   250K .......... .......... .......... .......... .......... 67%  559K 0s\n",
      "   300K .......... .......... .......... .......... .......... 78% 2.68M 0s\n",
      "   350K .......... .......... .......... .......... .......... 89% 1.74M 0s\n",
      "   400K .......... .......... .......... .......... ......    100% 9.42M=0.7s\n",
      "\n",
      "2021-11-04 23:22:38 (638 KB/s) - 'cooking.stackexchange.tar.gz.3' saved [457609/457609]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b0b344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?\n",
      "__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments\n",
      "__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?\n",
      "__label__restaurant Michelin Three Star Restaurant; but if the chef is not there\n",
      "__label__knife-skills __label__dicing Without knife skills, how can I quickly and accurately dice vegetables?\n",
      "__label__storage-method __label__equipment __label__bread What's the purpose of a bread box?\n",
      "__label__baking __label__food-safety __label__substitutions __label__peanuts how to seperate peanut oil from roasted peanuts at home?\n",
      "__label__chocolate American equivalent for British chocolate terms\n",
      "__label__baking __label__oven __label__convection Fan bake vs bake\n",
      "__label__sauce __label__storage-lifetime __label__acidity __label__mayonnaise Regulation and balancing of readymade packed mayonnaise and other sauces\n"
     ]
    }
   ],
   "source": [
    "!head cooking.stackexchange.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e8827",
   "metadata": {},
   "source": [
    "Each line of the text file contains a list of labels, followed by the corresponding document. All the labels start by the __label__ prefix, which is how fastText recognize what is a label or what is a word. The model is then trained to predict the labels given the word in the document.\n",
    "\n",
    "Before training our first classifier, we need to split the data into train and validation. We will use the validation set to evaluate how good the learned classifier is on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f955c0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  15404  169582 1401900 cooking.stackexchange.txt\n"
     ]
    }
   ],
   "source": [
    "!wc cooking.stackexchange.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0efc8",
   "metadata": {},
   "source": [
    "Our full dataset contains 15404 examples. Let's split it into a training set of 12404 examples and a validation set of 3000 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21da4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 12404 cooking.stackexchange.txt > cooking.train\n",
    "!tail -n 3000 cooking.stackexchange.txt > cooking.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021c1bb",
   "metadata": {},
   "source": [
    "## Our First Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1d1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbc624",
   "metadata": {},
   "source": [
    "The input argument indicates the file containing the training examples. We can now use the model variable to access information on the trained model.\n",
    "\n",
    "We can also call save_model to save it as a file and load it later with load_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79028bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_cooking.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ac9c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',), array([0.06506271]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Which baking dish is best to bake a banana bread ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f076186",
   "metadata": {},
   "source": [
    "The predicted tag is baking which fits well to this question. Let us now try a second example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "771795ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',), array([0.07080489]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Why not put knives in the dishwasher?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e39e2",
   "metadata": {},
   "source": [
    "The label predicted by the model is baking, which is not relevant. Somehow, the model seems to fail on simple examples.\n",
    "\n",
    "To get a better sense of its quality, let's test it on the validation data by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d46e0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.127, 0.05492287732449185)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"cooking.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352606d4",
   "metadata": {},
   "source": [
    "The output are the number of samples (here 3000), the precision at one (0.126) and the recall at one (0.0547).\n",
    "\n",
    "We can also compute the precision at five and recall at five with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d128ee",
   "metadata": {},
   "source": [
    "The precision is 0.117667 and the recall is at 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a679fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.06613333333333334, 0.14300129739080294)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"cooking.valid\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b88fe4",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add6ac8",
   "metadata": {},
   "source": [
    "Precision is the number of correct ones among the predicted ones. Recall is the number of labels that were predicted among the real labels. We shall use an example; \n",
    "<br>\n",
    "<em>Why not put knives in the dishwasher</em>\n",
    "<br>\n",
    "On the stack exchange this is labelled with three tags: <code>equipment, cleaning and knives</code>, and these can be predicted by these labels:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91653607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',\n",
       "  '__label__food-safety',\n",
       "  '__label__bread',\n",
       "  '__label__equipment',\n",
       "  '__label__substitutions'),\n",
       " array([0.07080489, 0.06346922, 0.03655997, 0.03551925, 0.0352584 ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Why not put knives in the dishwasher?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e32781",
   "metadata": {},
   "source": [
    "are food-safety, baking, equipment, substitutions and bread.\n",
    "Thus, one out of five labels predicted by the model is correct, giving a precision of 0.20. Out of the three real labels, only one is predicted by the model, giving a recall of 0.33."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635afaa6",
   "metadata": {},
   "source": [
    "## Making the model better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75eb81",
   "metadata": {},
   "source": [
    "The model obtained by running fastText with the default arguments is pretty bad at classifying new questions. Let's try to improve the performance, by changing the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5853a",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2571bb",
   "metadata": {},
   "source": [
    "Looking at the data, we observe that some words contain uppercase letter or punctuation. One of the first step to improve the performance of our model is to apply some simple pre-processing. A crude normalization can be obtained using command line tools such as sed and tr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dd0f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cooking.stackexchange.txt | sed -e \"s/\\([.\\!?,'/()]\\)/ \\1 /g\" | tr \"[:upper:]\" \"[:lower:]\" > cooking.preprocessed.txt\n",
    "!head -n 12404 cooking.preprocessed.txt > cooking.train\n",
    "!tail -n 3000 cooking.preprocessed.txt > cooking.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81949db7",
   "metadata": {},
   "source": [
    "Let's train a new model on the pre-processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f94d2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.14966666666666667, 0.06472538561337754)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\")\n",
    "model.test(\"cooking.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7a501",
   "metadata": {},
   "source": [
    "We observe that thanks to the pre-processing, the vocabulary is smaller (from 14k words to 9k). The precision is also starting to go up by 4%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab561a00",
   "metadata": {},
   "source": [
    "## more epochs and larger learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b5256",
   "metadata": {},
   "source": [
    "By default, fastText sees each training example only five times during training, which is pretty small, given that our training set only have 12k training examples. The number of times each examples is seen (also known as the number of epochs), can be increased using the -epoch option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb105bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\", epoch=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0d3f7",
   "metadata": {},
   "source": [
    "Let's test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774cb803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.5176666666666667, 0.22387199077410985)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"cooking.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbff2d9",
   "metadata": {},
   "source": [
    "This is much better! Another way to change the learning speed of our model is to increase (or decrease) the learning rate of the algorithm. This corresponds to how much the model changes after processing each example. A learning rate of 0 would mean that the model does not change at all, and thus, does not learn anything. Good values of the learning rate are in the range 0.1 - 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43e8b6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.5826666666666667, 0.25198212483782617)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\", lr=1.0)\n",
    "model.test(\"cooking.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ba26a",
   "metadata": {},
   "source": [
    "Even better! Let's try both together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7503043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\", lr=1.0, epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08a07688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.587, 0.2538561337754072)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"cooking.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ed74b",
   "metadata": {},
   "source": [
    "Let us now add a few more features to improve even further our performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88abeb",
   "metadata": {},
   "source": [
    "### using word n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b6358",
   "metadata": {},
   "source": [
    "Finally, we can improve the performance of a model by using word bigrams, instead of just unigrams. This is especially important for classification problems where word order is important, such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde4a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\", lr=1.0, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0fe2b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.6076666666666667, 0.26279371486233244)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"cooking.valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289cfe0d",
   "metadata": {},
   "source": [
    "With a few steps, we were able to go from a precision at one of 12.6% to 60%. Important steps included:\n",
    "<br>\n",
    "<ul>\n",
    "    <li>preprocessing the data</li>\n",
    "    <li>changing the number of epochs (using the option -epoch, standard range [5 - 50])</li>\n",
    "    <li>changing the learning rate (using the option -lr, standard range [0.1 - 1.0]) </li>\n",
    "    <li>using word n-grams (using the option -wordNgrams, standard range [1 - 5])</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028f434",
   "metadata": {},
   "source": [
    "## Hierarchical softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407e6f1",
   "metadata": {},
   "source": [
    "Since we are training our model on a few thousands of examples, the training only takes a few seconds. But training models on larger datasets, with more labels can start to be too slow. A potential solution to make the training faster is to use the hierarchical softmax, instead of the regular softmax. This can be done with the option -loss hs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ec5d258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 804 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(input=\"cooking.train\", lr=1.0,\n",
    "                    epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='hs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02321164",
   "metadata": {},
   "source": [
    "Training should now take less than a second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170dc9a8",
   "metadata": {},
   "source": [
    "The hierarchical softmax is a loss function that apprpximates the softmax with a much faster computation. It does this by using a binary tree that has leaves corresponding to the labels. Each intermediate node has a binary decsion activation ~ sigmoid that is trained and predicts if we should go to the left or right. The probability of output is given by the product of the probabilities of the intermediate nodes along the path from the roor to the output unit leaf. In fastText, we use a Huffman tree so that the lookup time is faster for ore frequent outputs and thus the avergae lookuptime for the output is optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8483085",
   "metadata": {},
   "source": [
    "### Multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105718e",
   "metadata": {},
   "source": [
    "When we want to assign a document to multiple labels, we can still use the softmax loss and play with the parameters for prediction, namely the number of labels to predict and the threshold for the predicted probability. However playing with these arguments can be tricky and unintuitive since the probabilities must sum to 1.\n",
    "\n",
    "A convenient way to handle multiple labels is to use independent binary classifiers for each label. This can be done with -loss one-vs-all or -loss ova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3619438",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = fasttext.train_supervised(input=\"cooking.train\", lr=0.5, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='ova')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac9c31",
   "metadata": {},
   "source": [
    "It is a good idea to decrease the learning rate compared to other loss functions.\n",
    "\n",
    "Now let's have a look on our predictions, we want as many prediction as possible (argument -1) and we want only labels with probability higher or equal to 0.5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b761eb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',\n",
       "  '__label__bread',\n",
       "  '__label__equipment',\n",
       "  '__label__bananas'),\n",
       " array([1.00001001, 0.98718882, 0.93629503, 0.91731268]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict(\"Which baking dish is best to bake a banana bread ?\", k=-1, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8ae40",
   "metadata": {},
   "source": [
    "We can also evaluate our results with the test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5692ee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.003146031746031746, 1.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.test(\"cooking.valid\", k=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4a259",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we gave a brief overview of how to use fastText to train powerful text classifiers. We had a light overview of some of the most important options to tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
